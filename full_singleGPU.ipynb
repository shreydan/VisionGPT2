{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **VisionGPT-2 Image Captioning Model**\n","---\n","\n","- *almost* built from scratch.\n","- pretrained weights loading via HF & timm\n","- dataset preparation from scratch as well."]},{"cell_type":"markdown","metadata":{},"source":["# Imports\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-09-27T06:36:29.887612Z","iopub.status.busy":"2023-09-27T06:36:29.887201Z","iopub.status.idle":"2023-09-27T06:36:29.894520Z","shell.execute_reply":"2023-09-27T06:36:29.893546Z","shell.execute_reply.started":"2023-09-27T06:36:29.887583Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from timm import create_model, list_models\n","from types import SimpleNamespace\n","from transformers import GPT2LMHeadModel, GPT2TokenizerFast, get_linear_schedule_with_warmup\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from PIL import Image\n","from pathlib import Path\n","from sklearn.model_selection import train_test_split\n","from torch.cuda.amp import GradScaler, autocast\n","from tqdm.auto import tqdm\n","import gc"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:29.901343Z","iopub.status.busy":"2023-09-27T06:36:29.900496Z","iopub.status.idle":"2023-09-27T06:36:29.909630Z","shell.execute_reply":"2023-09-27T06:36:29.908637Z","shell.execute_reply.started":"2023-09-27T06:36:29.901308Z"},"trusted":true},"outputs":[],"source":["%env TOKENIZERS_PARALLELISM = false"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset\n","---\n","- the dataset we're using is `Flick30k`, it has `158915` samples. It's actually about 30k images and 5 captions for each image.\n","- augmentations: PIL + albumentations. Fun fact: albumentations is a LOT faster than torchvision\n","- mean and std for ViT models is `[0.5,0.5,0.5]` unlike the standard ImageNet mean and std."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:29.913591Z","iopub.status.busy":"2023-09-27T06:36:29.911990Z","iopub.status.idle":"2023-09-27T06:36:29.923353Z","shell.execute_reply":"2023-09-27T06:36:29.922316Z","shell.execute_reply.started":"2023-09-27T06:36:29.913566Z"},"trusted":true},"outputs":[],"source":["sample_tfms = [\n","    A.HorizontalFlip(),\n","    A.OneOf([\n","        A.MotionBlur(p=.3),\n","        A.MedianBlur(blur_limit=3, p=0.3),\n","        A.Blur(blur_limit=3, p=0.5),\n","    ], p=0.3),\n","    A.RGBShift(),\n","    A.RandomBrightnessContrast(),\n","    A.RandomResizedCrop(384,384),\n","    A.ColorJitter(),\n","    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.3, rotate_limit=45, p=0.5),\n","    A.HueSaturationValue(p=0.3),\n","]\n","train_tfms = A.Compose([\n","    *sample_tfms,\n","    A.Resize(224,224),\n","    A.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5],always_apply=True),\n","    ToTensorV2()\n","])\n","valid_tfms = A.Compose([\n","    A.Resize(224,224),\n","    A.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5],always_apply=True),\n","    ToTensorV2()\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:29.926650Z","iopub.status.busy":"2023-09-27T06:36:29.925784Z","iopub.status.idle":"2023-09-27T06:36:30.486892Z","shell.execute_reply":"2023-09-27T06:36:30.485792Z","shell.execute_reply.started":"2023-09-27T06:36:29.926571Z"},"trusted":true},"outputs":[],"source":["tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.pad_token"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:30.488903Z","iopub.status.busy":"2023-09-27T06:36:30.488473Z","iopub.status.idle":"2023-09-27T06:36:30.496194Z","shell.execute_reply":"2023-09-27T06:36:30.495330Z","shell.execute_reply.started":"2023-09-27T06:36:30.488870Z"},"trusted":true},"outputs":[],"source":["tokenizer.encode_plus('hello! this is a caption')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:30.500173Z","iopub.status.busy":"2023-09-27T06:36:30.499453Z","iopub.status.idle":"2023-09-27T06:36:30.508704Z","shell.execute_reply":"2023-09-27T06:36:30.507865Z","shell.execute_reply.started":"2023-09-27T06:36:30.500143Z"},"trusted":true},"outputs":[],"source":["class Dataset:\n","    def __init__(self, df, tfms):\n","        self.df = df\n","        self.tfms = tfms\n","    def __len__(self):\n","        return len(self.df)\n","    def __getitem__(self,idx):\n","        sample = self.df.iloc[idx,:]\n","        image = sample['image']\n","        caption = sample['caption']\n","        image = Image.open(image).convert('RGB')\n","        image = np.array(image)\n","        augs = self.tfms(image=image)\n","        image = augs['image']\n","        caption = f\"{caption}<|endoftext|>\"\n","        input_ids = tokenizer(\n","            caption,\n","            truncation=True)['input_ids']\n","        labels = input_ids.copy()\n","        labels[:-1] = input_ids[1:]\n","        return image,input_ids,labels"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:30.512407Z","iopub.status.busy":"2023-09-27T06:36:30.512027Z","iopub.status.idle":"2023-09-27T06:36:32.218701Z","shell.execute_reply":"2023-09-27T06:36:32.217293Z","shell.execute_reply.started":"2023-09-27T06:36:30.512376Z"},"trusted":true},"outputs":[],"source":["base_path = Path('/kaggle/input/flickr30k/flickr30k_images')\n","df = pd.read_csv('/kaggle/input/flickr30k/captions.txt',delimiter=',')\n","df.rename({'image_name':'image','comment': 'caption'},inplace=True,axis=1)\n","df['image'] = df['image'].map(lambda x:base_path / x.strip())\n","df['caption'] = df['caption'].map(lambda x:x.strip().lower())\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:32.220831Z","iopub.status.busy":"2023-09-27T06:36:32.220387Z","iopub.status.idle":"2023-09-27T06:36:32.256739Z","shell.execute_reply":"2023-09-27T06:36:32.255686Z","shell.execute_reply.started":"2023-09-27T06:36:32.220797Z"},"trusted":true},"outputs":[],"source":["train_df, val_df = train_test_split(df,test_size=0.1)\n","train_df.reset_index(drop=True,inplace=True)\n","val_df.reset_index(drop=True,inplace=True)\n","len(train_df),len(val_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:32.258834Z","iopub.status.busy":"2023-09-27T06:36:32.258478Z","iopub.status.idle":"2023-09-27T06:36:32.263973Z","shell.execute_reply":"2023-09-27T06:36:32.263062Z","shell.execute_reply.started":"2023-09-27T06:36:32.258803Z"},"trusted":true},"outputs":[],"source":["train_ds = Dataset(train_df,train_tfms)\n","val_ds = Dataset(val_df,valid_tfms)"]},{"cell_type":"markdown","metadata":{},"source":["## Custom collate function\n","---\n","\n","- allows for dynamic padding so the model doesn't have to process `max_len` sequences which would be just filled with pad tokens\n","- instead, every batch is padded according to the longest sequence in the batch"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:32.266165Z","iopub.status.busy":"2023-09-27T06:36:32.265503Z","iopub.status.idle":"2023-09-27T06:36:32.274188Z","shell.execute_reply":"2023-09-27T06:36:32.273492Z","shell.execute_reply.started":"2023-09-27T06:36:32.266135Z"},"trusted":true},"outputs":[],"source":["def collate_fn(batch):\n","    image = [i[0] for i in batch]\n","    input_ids = [i[1] for i in batch]\n","    labels = [i[2] for i in batch]\n","    image = torch.stack(image,dim=0)\n","    input_ids = tokenizer.pad(\n","        {'input_ids':input_ids},\n","        padding='longest',\n","        return_attention_mask=False,\n","        return_tensors='pt'\n","    )['input_ids']\n","    labels = tokenizer.pad(\n","        {'input_ids':labels},\n","        padding='longest',\n","        return_attention_mask=False,\n","        return_tensors='pt'\n","    )['input_ids']\n","    mask = (input_ids!=tokenizer.pad_token_id).long()\n","    labels[mask==0]=-100\n","    return image, input_ids, labels"]},{"cell_type":"markdown","metadata":{},"source":["## How the data looks:\n","---\n","- every caption is a sequence of tokens, and as it is causal language modeling where the model predicts the next token, the labels are right shifted by 1 position.\n","- every caption ends with the end of sentence token: eos_token (50256 : <|endoftext|>)\n","- in GPT models, the pad tokens are same as the eos tokens, hence we also mask the pad tokens in the labels with -100 which are ignored by cross-entropy loss' default behaviour -- check `collate_fn` to see how I masked them. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:32.276230Z","iopub.status.busy":"2023-09-27T06:36:32.275598Z","iopub.status.idle":"2023-09-27T06:36:32.360675Z","shell.execute_reply":"2023-09-27T06:36:32.359221Z","shell.execute_reply.started":"2023-09-27T06:36:32.276198Z"},"trusted":true},"outputs":[],"source":["dl = torch.utils.data.DataLoader(train_ds,shuffle=True,batch_size=2,collate_fn=collate_fn)\n","_,c,l = next(iter(dl))\n","print(c[0])\n","print(l[0])"]},{"cell_type":"markdown","metadata":{},"source":["# Model\n","---\n","\n","- GPT2 on its own is a decoder-only model. So it understands only textual context. To create a model which also understands image context and applies it to the text, we use cross-attention.\n","- Cross Attention: It is the second attention layer of the decoder block as per the [classic transformer encoder-decoder model](https://www.arxiv-vanity.com/papers/1706.03762/)\n","- The query in cross-attention is the output of the previous causal attention layer in the decoder block. The key and value are the outputs of the corresponding encoder block i.e. the image context.\n","- Since GPT2 doesn't have a cross-attention layer, I coded it as `GPT2CrossAttention` and made it a part of the usual decoder `GPT2Block`.\n","- The encoder I used was ViT-base with patch_size=16 grabbed from `timm`.\n","- ViT and GPT2 are extremely compatible with each other. They have the same amount of depth i.e. encoder, decoder blocks respectively. They have the same hidden size of 768 as well. So I didn't have to pool the encoder outputs to match the decoder hidden size. Less parameters yay!\n","- GPT2 code I wrote was ofc inspired heavily by [NanoGPT](https://github.com/karpathy/nanoGPT)\n","\n","### This is somewhat how the architecture looks now.\n","![](https://i.imgur.com/fk68DMo.jpg)\n","*I drew this :)*"]},{"cell_type":"markdown","metadata":{},"source":["## Causal Attention Block\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:32.362526Z","iopub.status.busy":"2023-09-27T06:36:32.362154Z","iopub.status.idle":"2023-09-27T06:36:32.375632Z","shell.execute_reply":"2023-09-27T06:36:32.374555Z","shell.execute_reply.started":"2023-09-27T06:36:32.362484Z"},"trusted":true},"outputs":[],"source":["class GPT2Attention(nn.Module):\n","    def __init__(self,config):\n","        super().__init__()\n","        self.embed_dim = config.embed_dim\n","        self.n_heads = config.num_heads\n","        assert self.embed_dim % self.n_heads == 0, 'embedding dimension by be divisible by number of heads'\n","        self.head_size = self.embed_dim // self.n_heads\n","        self.seq_len = config.seq_len\n","        \n","        self.c_attn = nn.Linear(self.embed_dim, self.head_size * self.n_heads * 3,bias=True)\n","        self.scale = self.head_size ** -0.5\n","        \n","        self.register_buffer('mask',torch.tril(torch.ones(1,1,self.seq_len,self.seq_len)))\n","        \n","        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n","        \n","        self.attn_dropout = nn.Dropout(config.attention_dropout)\n","        self.resid_dropout = nn.Dropout(config.residual_dropout)\n","        \n","        \n","    def forward(self, x):\n","        b,t,c = x.shape\n","        # q,k,v shape individually: batch_size x seq_len x embed_dim\n","        # we know that qk_t = q x k_t, where q=bxtxhead_dim, k_t=bxhead_timxt\n","        q,k,v = self.c_attn(x).chunk(3,dim=-1)\n","        q = q.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3) # batch x n_heads x seq_len x head_dim\n","        k = k.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n","        v = v.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n","        \n","        qk_t = (q@k.transpose(-2,-1)) * self.scale\n","        qk_t = qk_t.masked_fill(self.mask[:,:,:t,:t]==0,float('-inf'))\n","        qk_t = F.softmax(qk_t,dim=-1)\n","        weights = self.attn_dropout(qk_t)\n","        \n","        attention = weights @ v # batch x n_heads x t x head_size\n","        attention = attention.permute(0,2,1,3).contiguous().view(b,t,c) # batch x t x embed_dim\n","        \n","        out = self.c_proj(attention)\n","        out = self.resid_dropout(out)\n","        \n","        return out"]},{"cell_type":"markdown","metadata":{},"source":["## Cross Attention Block\n","---\n","- Notice how I initialized the parameters with mean=0. and std=0.02. This is an OpenAI trick they used while training GPT2 as well."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:32.377746Z","iopub.status.busy":"2023-09-27T06:36:32.377399Z","iopub.status.idle":"2023-09-27T06:36:32.393228Z","shell.execute_reply":"2023-09-27T06:36:32.392279Z","shell.execute_reply.started":"2023-09-27T06:36:32.377714Z"},"trusted":true},"outputs":[],"source":["class GPT2CrossAttention(nn.Module):\n","    def __init__(self,config):\n","        super().__init__()\n","        self.embed_dim = config.embed_dim\n","        self.n_heads = config.num_heads\n","        assert self.embed_dim % self.n_heads == 0, 'embedding dimension by be divisible by number of heads'\n","        self.head_size = self.embed_dim // self.n_heads\n","        self.seq_len = config.seq_len\n","        \n","        self.q = nn.Linear(self.embed_dim,self.embed_dim)\n","        self.k = nn.Linear(self.embed_dim,self.embed_dim)\n","        self.v = nn.Linear(self.embed_dim,self.embed_dim)\n","        self.scale = self.head_size ** -0.5\n","        \n","        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n","        \n","        self.attn_dropout = nn.Dropout(config.attention_dropout)\n","        self.resid_dropout = nn.Dropout(config.residual_dropout)\n","        \n","        self.apply(self._init_weights)\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        \n","        \n","    def forward(self, q,k,v):\n","        b,t,c = q.shape\n","        \n","        q = self.q(q)\n","        k = self.k(k)\n","        v = self.v(v)\n","        \n","        q = q.view(b,q.size(1),self.n_heads,self.head_size).permute(0,2,1,3) # batch x n_heads x seq_len x head_dim\n","        k = k.view(b,k.size(1),self.n_heads,self.head_size).permute(0,2,1,3)\n","        v = v.view(b,v.size(1),self.n_heads,self.head_size).permute(0,2,1,3)\n","        \n","        qk_t = (q@k.transpose(-2,-1)) * self.scale\n","        qk_t = F.softmax(qk_t,dim=-1)\n","        weights = self.attn_dropout(qk_t)\n","        \n","        attention = weights @ v # batch x n_heads x t x head_size\n","        attention = attention.permute(0,2,1,3).contiguous().view(b,t,c) # batch x t x embed_dim\n","        \n","        out = self.c_proj(attention)\n","        out = self.resid_dropout(out)\n","        \n","        return out"]},{"cell_type":"markdown","metadata":{},"source":["## Feed Forward Block\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:32.395071Z","iopub.status.busy":"2023-09-27T06:36:32.394734Z","iopub.status.idle":"2023-09-27T06:36:32.404064Z","shell.execute_reply":"2023-09-27T06:36:32.403373Z","shell.execute_reply.started":"2023-09-27T06:36:32.395040Z"},"trusted":true},"outputs":[],"source":["class GPT2MLP(nn.Module):\n","    def __init__(self,config):\n","        super().__init__()\n","        self.embed_dim = config.embed_dim\n","        self.mlp_ratio = config.mlp_ratio\n","        self.mlp_dropout = config.mlp_dropout\n","        \n","        self.c_fc = nn.Linear(self.embed_dim,self.embed_dim*self.mlp_ratio)\n","        self.c_proj = nn.Linear(self.embed_dim*self.mlp_ratio,self.embed_dim)\n","        self.act = nn.GELU()\n","        self.dropout = nn.Dropout(self.mlp_dropout)\n","        \n","    def forward(self,x):\n","        x = self.c_fc(x)\n","        x = self.act(x)\n","        x = self.c_proj(x)\n","        x = self.dropout(x)\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["## Decoder Block\n","---\n","- with added cross-attention and pre-normalization"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:32.410217Z","iopub.status.busy":"2023-09-27T06:36:32.409138Z","iopub.status.idle":"2023-09-27T06:36:32.417659Z","shell.execute_reply":"2023-09-27T06:36:32.416743Z","shell.execute_reply.started":"2023-09-27T06:36:32.410186Z"},"trusted":true},"outputs":[],"source":["class GPT2Block(nn.Module):\n","    def __init__(self,config):\n","        super().__init__()\n","        self.embed_dim = config.embed_dim\n","        self.ln_1 = nn.LayerNorm(self.embed_dim)\n","        self.attn = GPT2Attention(config)\n","        self.ln_2 = nn.LayerNorm(self.embed_dim)\n","        self.mlp = GPT2MLP(config)\n","        self.ln_3 = nn.LayerNorm(self.embed_dim)\n","        self.cross_attn = GPT2CrossAttention(config)\n","        \n","    def forward(self,x,enc_out):\n","        x = x+self.attn(self.ln_1(x))\n","        x = x+self.cross_attn(self.ln_2(x),enc_out,enc_out)\n","        x = x+self.mlp(self.ln_3(x))\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["## The main model\n","---\n","- creating ViT model via timm\n","- added embedding code and right forward pass for the ViT model as per [their code.](https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py)\n","- each encoder output is passed to the decoder input and to the next encoder input as well.\n","- the final decoder outputs are passed through a head block to generate logits as per the vocab size.\n","- loss is calculated using cross-entropy if labels are present else, logits for the final token in the sequence are returned for generation.\n","- `pretrained_layers_trainable`: freezes/unfreezes ViT and GPT2 pretrained layers\n","- `unfreeze_gpt_layers`: unfreezes GPT2 layers only\n","- `from_pretrained`: loads GPT2 weights via huggingface gpt2\n","- `generate`: generates caption, sampling via `torch.multinomial` or deterministic via `argmax` with `temperature` control."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:32.430824Z","iopub.status.busy":"2023-09-27T06:36:32.430084Z","iopub.status.idle":"2023-09-27T06:36:32.461289Z","shell.execute_reply":"2023-09-27T06:36:32.460315Z","shell.execute_reply.started":"2023-09-27T06:36:32.430769Z"},"trusted":true},"outputs":[],"source":["class VisionGPT2Model(nn.Module):\n","    def __init__(self,config):\n","        super().__init__()\n","        \n","        self.config = config\n","        \n","        vit = create_model('vit_base_patch16_224',pretrained=True,num_classes=0)\n","        self.patch_embed = vit.patch_embed\n","        num_patches = self.patch_embed.num_patches\n","        \n","        self.cls_token = vit.cls_token\n","        embed_len = num_patches + vit.num_prefix_tokens\n","        self.pos_embed = vit.pos_embed\n","        self.pos_drop = nn.Dropout(p=0.)\n","        \n","        self.blocks = nn.ModuleList([vit.blocks[i] for i in range(config.depth)])\n","        \n","        self.transformer = nn.ModuleDict(dict(\n","            wte = nn.Embedding(config.vocab_size,config.embed_dim),\n","            wpe = nn.Embedding(config.seq_len,config.embed_dim),\n","            drop = nn.Dropout(config.emb_dropout),\n","            h = nn.ModuleList([GPT2Block(config) for _ in range(config.depth)]),\n","            ln_f = nn.LayerNorm(config.embed_dim)\n","        ))\n","        self.lm_head = nn.Linear(config.embed_dim,config.vocab_size,bias=False)\n","        self.transformer.wte.weight = self.lm_head.weight\n","        \n","    def _pos_embed(self,x):\n","        pos_embed = self.pos_embed\n","        x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n","        x = x + pos_embed\n","        return self.pos_drop(x)\n","    \n","    def pretrained_layers_trainable(self,trainable=False):\n","        layers = [\n","            self.cls_token, self.patch_embed, self.pos_embed, self.blocks,\n","            self.transformer.wte, self.transformer.wpe,\n","            self.transformer.ln_f, self.lm_head\n","        ]\n","        gpt_layers = [[\n","            self.transformer.h[i].ln_1,self.transformer.h[i].ln_2,\n","            self.transformer.h[i].attn,self.transformer.h[i].mlp\n","        ] for i in range(self.config.depth)]\n","        for l in gpt_layers:\n","            layers.extend(l)\n","        \n","        for layer in layers:\n","            if not isinstance(layer,nn.Parameter):\n","                for p in layer.parameters():\n","                    p.requires_grad = trainable\n","            else:\n","                layer.requires_grad = trainable\n","                \n","        total_frozen_params = sum([p.numel() for p in self.parameters() if not p.requires_grad])\n","        print(f'{total_frozen_params=}')\n","        \n","    def unfreeze_gpt_layers(self,):\n","        gpt_layers = [[\n","            self.transformer.h[i].ln_1,self.transformer.h[i].ln_2,\n","            self.transformer.h[i].attn,self.transformer.h[i].mlp\n","        ] for i in range(self.config.depth)]\n","        flatten = []\n","        for l in gpt_layers:\n","            flatten.extend(l)\n","            \n","        for layer in flatten:\n","            if not isinstance(layer,nn.Parameter):\n","                for p in layer.parameters():\n","                    p.requires_grad = True\n","            else:\n","                layer.requires_grad = True\n","        \n","    @classmethod    \n","    def from_pretrained(self,config):\n","        model = VisionGPT2Model(config)\n","        sd = model.state_dict()\n","        keys = sd.keys()\n","        ignore_matches = ['blocks.','cross_attn.','ln_3','cls_token','pos_embed','patch_embed.','.attn.mask']\n","        vit_keys = [key for key in keys if any(match in key for match in ignore_matches)]\n","        gpt_keys = [key for key in keys if key not in vit_keys]\n","        \n","        gpt2_small = GPT2LMHeadModel.from_pretrained('gpt2')\n","        sd_hf = gpt2_small.state_dict()\n","        hf_keys = sd_hf.keys()\n","        hf_keys = [k for k in hf_keys if not k.endswith('.attn.masked_bias')]\n","        hf_keys = [k for k in hf_keys if not k.endswith('.attn.bias')]\n","        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n","        \n","        for k in hf_keys:\n","            if any(match in k for match in ignore_matches):\n","                continue\n","            if any(k.endswith(w) for w in transposed):\n","                assert sd_hf[k].shape[::-1] == sd[k].shape\n","                with torch.no_grad():\n","                    sd[k].copy_(sd_hf[k].t())\n","            else:\n","                assert sd_hf[k].shape == sd[k].shape\n","                with torch.no_grad():\n","                    sd[k].copy_(sd_hf[k])\n","            \n","        model.load_state_dict(sd)\n","        \n","        return model\n","    \n","    def forward(self,image,input_ids,labels=None):\n","        \n","        image = self.patch_embed(image)\n","        image = self._pos_embed(image)\n","        \n","        token_embeddings = self.transformer.wte(input_ids) # batch x seq_len\n","        pos_embs = torch.arange(0,input_ids.size(1)).to(input_ids.device)\n","        positional_embeddings = self.transformer.wpe(pos_embs)\n","        input_ids = self.transformer.drop(token_embeddings+positional_embeddings)\n","        \n","        for i in range(self.config.depth):\n","            image = self.blocks[i](image)\n","            input_ids = self.transformer.h[i](input_ids, image)\n","        \n","        input_ids = self.transformer.ln_f(input_ids)\n","        \n","        if labels is not None:\n","            lm_logits = self.lm_head(input_ids)\n","            loss = F.cross_entropy(lm_logits.view(-1, lm_logits.shape[-1]), labels.view(-1))\n","            return loss\n","        \n","        lm_logits = self.lm_head(input_ids[:,[-1],:])\n","        return lm_logits\n","    \n","    def generate(self,image,sequence,max_tokens=50,temperature=1.0,deterministic=False):\n","        for _ in range(max_tokens):\n","            out = self(image,sequence)\n","            out = out[:,-1,:] / temperature\n","            probs = F.softmax(out,dim=-1)\n","            if deterministic:\n","                next_token = torch.argmax(probs,dim=-1,keepdim=True)\n","            else:\n","                next_token = torch.multinomial(probs,num_samples=1)\n","            sequence = torch.cat([sequence,next_token],dim=1)\n","            if next_token.item() == tokenizer.eos_token_id:\n","                break\n","            \n","        return sequence.cpu().flatten()"]},{"cell_type":"markdown","metadata":{},"source":["# Training\n","---\n","- the pretrained layers are initially frozen as I need to train the cross attention layers first\n","- in the following epochs, GPT2 is unfreezed and trained, in the final few epochs, the ViT is unfreezed as well.\n","- optimizer: Adam\n","- scheduler: OneCycleLR\n","- mixed-precision fp16 training with autocast and grad-scaler in torch\n","- metrics: cross-entropy loss and perplexity = e^loss, both lower the better\n","- best model is saved based on validation perplexity and the same is loaded while generating captions.\n","\n","### Generation\n","\n","- GPT2 generally requires context before generating anything, since for image captioning we can't really provide an initial context except the image itself, the initial context we provide to GPT is just `[50256]` i.e `<|endoftext|>` which is also the beginning of sentence token in GPT. For other models such as OPT it is `</s>`. This one token acts as the initial token."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:32.464621Z","iopub.status.busy":"2023-09-27T06:36:32.464257Z","iopub.status.idle":"2023-09-27T06:36:32.493149Z","shell.execute_reply":"2023-09-27T06:36:32.492142Z","shell.execute_reply.started":"2023-09-27T06:36:32.464597Z"},"trusted":true},"outputs":[],"source":["class Trainer:\n","    def __init__(self,model_config,train_config, dls):\n","        \n","        self.train_config = train_config\n","        self.model_config = model_config\n","        self.device = self.train_config.device\n","        \n","        self.model = VisionGPT2Model.from_pretrained(model_config).to(self.device)\n","        self.model.pretrained_layers_trainable(trainable=False)\n","        \n","        print(f'trainable parameters: {sum([p.numel() for p in self.model.parameters() if p.requires_grad])}')\n","        \n","        self.tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n","        self.tokenizer.pad_token = self.tokenizer.eos_token\n","        \n","        self.scaler = GradScaler()\n","        \n","        self.train_dl, self.val_dl = dls\n","        \n","        total_steps = len(self.train_dl)\n","        \n","        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.train_config.lr / 25.)\n","        self.sched = torch.optim.lr_scheduler.OneCycleLR(\n","            self.optim,\n","            max_lr=self.train_config.lr,\n","            epochs=self.train_config.epochs,\n","            steps_per_epoch=total_steps\n","        )\n","        \n","#         self.sched = get_linear_schedule_with_warmup(self.optim,num_warmup_steps=0,num_training_steps=total_steps)\n","        \n","        self.metrics = pd.DataFrame()\n","        self.metrics[['train_loss','train_perplexity','val_loss','val_perplexity']] = None\n","        \n","        self.gen_tfms = A.Compose([\n","            A.Resize(224,224),\n","            A.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5],always_apply=True),\n","            ToTensorV2()\n","        ])\n","            \n","        \n","    def save_model(self,):\n","        self.train_config.model_path.mkdir(exist_ok=True)\n","        sd = self.model.state_dict()\n","        torch.save(sd,self.train_config.model_path/'captioner.pt')\n","        \n","        \n","    def load_best_model(self,):\n","        sd = torch.load(self.train_config.model_path/'captioner.pt')\n","        self.model.load_state_dict(sd)\n","    \n","    \n","    def train_one_epoch(self,epoch):\n","        \n","        prog = tqdm(self.train_dl,total=len(self.train_dl))\n","        \n","        running_loss = 0.\n","        \n","        for image, input_ids, labels in prog:\n","            \n","            with autocast():\n","                image = image.to(self.device)\n","                input_ids = input_ids.to(self.device)\n","                labels = labels.to(self.device)\n","                \n","                loss = self.model(image,input_ids,labels)\n","                \n","                self.scaler.scale(loss).backward()\n","                self.scaler.step(self.optim)\n","                self.scaler.update()\n","                self.sched.step()\n","                self.optim.zero_grad(set_to_none=True)\n","                \n","                running_loss += loss.item()\n","                \n","                prog.set_description(f'train loss: {loss.item():.3f}')\n","                \n","            del image, input_ids, labels, loss\n","            \n","        train_loss = running_loss / len(self.train_dl)\n","        train_pxp = np.exp(train_loss)\n","        \n","        self.metrics.loc[epoch,['train_loss','train_perplexity']] = (train_loss,train_pxp)\n","        \n","        \n","    @torch.no_grad()\n","    def valid_one_epoch(self,epoch):\n","        \n","        prog = tqdm(self.val_dl,total=len(self.val_dl))\n","        \n","        running_loss = 0.\n","        \n","        for image, input_ids, labels in prog:\n","            \n","            with autocast():\n","                image = image.to(self.device)\n","                input_ids = input_ids.to(self.device)\n","                labels = labels.to(self.device)\n","                \n","                loss = self.model(image,input_ids,labels)\n","                running_loss += loss.item()\n","                \n","                prog.set_description(f'valid loss: {loss.item():.3f}')\n","                \n","            del image, input_ids, labels, loss\n","            \n","        val_loss = running_loss / len(self.val_dl)\n","        val_pxp = np.exp(val_loss)\n","        \n","        self.metrics.loc[epoch,['val_loss','val_perplexity']] = (val_loss,val_pxp)\n","        \n","        return val_pxp\n","        \n","        \n","    def clean(self):\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","       \n","    \n","    def fit(self,):\n","        \n","        best_pxp = 1e9\n","        best_epoch = -1\n","        prog = tqdm(range(self.train_config.epochs))\n","        \n","        for epoch in prog:\n","            \n","            if epoch == self.train_config.freeze_epochs_gpt:\n","                self.model.unfreeze_gpt_layers()\n","                print('unfreezing GPT2 entirely...')\n","                \n","            if epoch == self.train_config.freeze_epochs_all:\n","                self.model.pretrained_layers_trainable(trainable=True)\n","            \n","            self.model.train()\n","            prog.set_description('training')\n","            self.train_one_epoch(epoch)\n","            self.clean()\n","            \n","            self.model.eval()\n","            prog.set_description('validating')\n","            pxp = self.valid_one_epoch(epoch)\n","            self.clean()\n","            \n","            print(self.metrics.tail(1))\n","            \n","            if pxp < best_pxp:\n","                best_pxp = pxp\n","                best_epoch = epoch\n","                print('saving best model...')\n","                self.save_model()\n","                \n","        return {\n","            'best_perplexity': best_pxp,\n","            'best_epoch': best_epoch\n","        }\n","           \n","        \n","    @torch.no_grad()\n","    def generate_caption(self,image,max_tokens=50,temperature=1.0,deterministic=False):\n","        \n","        self.model.eval()\n","        \n","        image = Image.open(image).convert('RGB')\n","        image = np.array(image)\n","        image = self.gen_tfms(image=image)['image']\n","        image = image.unsqueeze(0).to(self.device)\n","        sequence = torch.ones(1,1).to(device=self.device).long() * self.tokenizer.bos_token_id\n","        \n","        caption = self.model.generate(\n","            image,\n","            sequence,\n","            max_tokens=max_tokens,\n","            temperature=temperature,\n","            deterministic=deterministic\n","        )\n","        caption = self.tokenizer.decode(caption.numpy(),skip_special_tokens=True)\n","        \n","        return caption"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:32.495776Z","iopub.status.busy":"2023-09-27T06:36:32.495097Z","iopub.status.idle":"2023-09-27T06:36:32.505104Z","shell.execute_reply":"2023-09-27T06:36:32.504120Z","shell.execute_reply.started":"2023-09-27T06:36:32.495744Z"},"trusted":true},"outputs":[],"source":["model_config = SimpleNamespace(\n","    vocab_size = 50_257,\n","    embed_dim = 768,\n","    num_heads = 12,\n","    seq_len = 1024,\n","    depth = 12,\n","    attention_dropout = 0.1,\n","    residual_dropout = 0.1,\n","    mlp_ratio = 4,\n","    mlp_dropout = 0.1,\n","    emb_dropout = 0.1,\n",")\n","train_config = SimpleNamespace(\n","    epochs = 7,\n","    freeze_epochs_gpt = 1,\n","    freeze_epochs_all = 2,\n","    lr = 5e-5,\n","    device = 'cuda',\n","    model_path = Path('captioner'),\n","    batch_size = 32\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:32.507109Z","iopub.status.busy":"2023-09-27T06:36:32.506737Z","iopub.status.idle":"2023-09-27T06:36:32.515883Z","shell.execute_reply":"2023-09-27T06:36:32.514940Z","shell.execute_reply.started":"2023-09-27T06:36:32.507079Z"},"trusted":true},"outputs":[],"source":["train_dl = torch.utils.data.DataLoader(train_ds,batch_size=train_config.batch_size,shuffle=True,pin_memory=True,num_workers=2,persistent_workers=True,collate_fn=collate_fn)\n","val_dl = torch.utils.data.DataLoader(val_ds,batch_size=train_config.batch_size,shuffle=False,pin_memory=True,num_workers=2,persistent_workers=True,collate_fn=collate_fn)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:32.517740Z","iopub.status.busy":"2023-09-27T06:36:32.517357Z","iopub.status.idle":"2023-09-27T06:37:08.792377Z","shell.execute_reply":"2023-09-27T06:37:08.791224Z","shell.execute_reply.started":"2023-09-27T06:36:32.517711Z"},"trusted":true},"outputs":[],"source":["trainer = Trainer(model_config,train_config,(train_dl,val_dl))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:37:08.796343Z","iopub.status.busy":"2023-09-27T06:37:08.796031Z"},"trusted":true},"outputs":[],"source":["trainer.fit()"]},{"cell_type":"markdown","metadata":{},"source":["# Results\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.plot(trainer.metrics['train_loss'],color='red',label='train loss')\n","plt.plot(trainer.metrics['val_loss'],color='orange',label='valid loss')\n","plt.title('loss, lower=better')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.plot(trainer.metrics['train_perplexity'],color='blue',label='train perplexity')\n","plt.plot(trainer.metrics['val_perplexity'],color='lightblue',label='valid perplexity')\n","plt.title('perplexity, lower=better')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Predictions\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.load_best_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for i in range(50):\n","    det = False\n","    test = val_df.sample(n=1).values[0]\n","    test_img, test_caption = test[0],test[1]\n","    plt.imshow(Image.open(test_img).convert('RGB'))\n","    if i>=24:\n","        det=True\n","    gen_caption = trainer.generate_caption(test_img,temperature=1.0,deterministic=det)\n","    plt.title(f\"actual: {test_caption}\\nmodel: {gen_caption}\\ntemp: {1.0} deterministic generation: {det}\")\n","    plt.axis('off')\n","    plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
