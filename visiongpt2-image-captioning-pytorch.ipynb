{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **VisionGPT-2 Image Captioning Model**\n---\n\n- *almost* built from scratch.\n- pretrained weights loading via HF & timm\n- dataset preparation from scratch as well.","metadata":{}},{"cell_type":"markdown","source":"# Imports\n---","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom timm import create_model, list_models\nfrom types import SimpleNamespace\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast, get_linear_schedule_with_warmup\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom PIL import Image\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom torch.cuda.amp import GradScaler, autocast\nfrom tqdm.auto import tqdm\nimport gc\nimport json","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2023-10-02T07:55:08.381194Z","iopub.execute_input":"2023-10-02T07:55:08.381817Z","iopub.status.idle":"2023-10-02T07:55:23.614102Z","shell.execute_reply.started":"2023-10-02T07:55:08.381781Z","shell.execute_reply":"2023-10-02T07:55:23.613164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%env TOKENIZERS_PARALLELISM = false","metadata":{"execution":{"iopub.status.busy":"2023-10-01T15:30:46.139934Z","iopub.execute_input":"2023-10-01T15:30:46.140927Z","iopub.status.idle":"2023-10-01T15:30:46.148164Z","shell.execute_reply.started":"2023-10-01T15:30:46.140888Z","shell.execute_reply":"2023-10-01T15:30:46.147285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset\n---\n- the dataset we're using is `COCO 2017`, it has about 500k samples, we will only use 150k samples. \n- augmentations: PIL + albumentations. Fun fact: albumentations is a LOT faster than torchvision\n- mean and std for ViT models is `[0.5,0.5,0.5]` unlike the standard ImageNet mean and std.","metadata":{}},{"cell_type":"code","source":"sample_tfms = [\n    A.HorizontalFlip(),\n    A.RandomBrightnessContrast(),\n    A.ColorJitter(),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.3, rotate_limit=45, p=0.5),\n    A.HueSaturationValue(p=0.3),\n]\ntrain_tfms = A.Compose([\n    *sample_tfms,\n    A.Resize(224,224),\n    A.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5],always_apply=True),\n    ToTensorV2()\n])\nvalid_tfms = A.Compose([\n    A.Resize(224,224),\n    A.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5],always_apply=True),\n    ToTensorV2()\n])","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:55:47.552804Z","iopub.execute_input":"2023-10-02T07:55:47.553454Z","iopub.status.idle":"2023-10-02T07:55:47.561038Z","shell.execute_reply.started":"2023-10-02T07:55:47.553424Z","shell.execute_reply":"2023-10-02T07:55:47.559603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.pad_token","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:55:47.994750Z","iopub.execute_input":"2023-10-02T07:55:47.995414Z","iopub.status.idle":"2023-10-02T07:55:49.593725Z","shell.execute_reply.started":"2023-10-02T07:55:47.995385Z","shell.execute_reply":"2023-10-02T07:55:49.592682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.encode_plus('hello! this is a caption')","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:55:49.595638Z","iopub.execute_input":"2023-10-02T07:55:49.596178Z","iopub.status.idle":"2023-10-02T07:55:49.612065Z","shell.execute_reply.started":"2023-10-02T07:55:49.596146Z","shell.execute_reply":"2023-10-02T07:55:49.610991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset:\n    def __init__(self, df, tfms):\n        self.df = df\n        self.tfms = tfms\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self,idx):\n        sample = self.df.iloc[idx,:]\n        image = sample['image']\n        caption = sample['caption']\n        image = Image.open(image).convert('RGB')\n        image = np.array(image)\n        augs = self.tfms(image=image)\n        image = augs['image']\n        caption = f\"{caption}<|endoftext|>\"\n        input_ids = tokenizer(\n            caption,\n            truncation=True)['input_ids']\n        labels = input_ids.copy()\n        labels[:-1] = input_ids[1:]\n        return image,input_ids,labels","metadata":{"execution":{"iopub.status.busy":"2023-10-02T07:55:49.614186Z","iopub.execute_input":"2023-10-02T07:55:49.614740Z","iopub.status.idle":"2023-10-02T07:55:49.621673Z","shell.execute_reply.started":"2023-10-02T07:55:49.614710Z","shell.execute_reply":"2023-10-02T07:55:49.620869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# COCO 2017\nbase_path = Path('/kaggle/input/coco-2017-dataset/coco2017')\nannot = base_path / 'annotations' / 'captions_train2017.json'\nwith open(annot, 'r') as f:\n    data = json.load(f)\n    data = data['annotations']\n\nsamples = []\n\nfor sample in data:\n    im = '%012d.jpg' % sample['image_id']\n    samples.append([im, sample['caption']])\n\ndf = pd.DataFrame(samples, columns=['image', 'caption'])\ndf['image'] = df['image'].apply(\n    lambda x: base_path / 'train2017' / x\n)\ndf = df.sample(150_000)\ndf = df.reset_index(drop=True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T08:00:47.961730Z","iopub.execute_input":"2023-10-02T08:00:47.962064Z","iopub.status.idle":"2023-10-02T08:00:56.641635Z","shell.execute_reply.started":"2023-10-02T08:00:47.962037Z","shell.execute_reply":"2023-10-02T08:00:56.640642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### some samples from the dataset","metadata":{}},{"cell_type":"code","source":"sampled_df = df.sample(n=20)\nfig, axs = plt.subplots(10, 2, figsize=(20, 30))\n\nfor i, row in enumerate(sampled_df.iterrows()):\n    ax = axs[i // 2, i % 2]\n    image_path = row[1]['image']\n    caption = row[1]['caption']\n    image = Image.open(image_path)\n    ax.imshow(image)\n    ax.axis('off')\n    ax.set_title(caption)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T08:12:29.382524Z","iopub.execute_input":"2023-10-02T08:12:29.382841Z","iopub.status.idle":"2023-10-02T08:12:32.614557Z","shell.execute_reply.started":"2023-10-02T08:12:29.382816Z","shell.execute_reply":"2023-10-02T08:12:32.613372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### you can also choose to use Flickr30k which has ~160k samples","metadata":{}},{"cell_type":"code","source":"# flickr30k\n\"\"\"\nbase_path = Path('/kaggle/input/flickr30k/flickr30k_images')\ndf = pd.read_csv('/kaggle/input/flickr30k/captions.txt',delimiter=',')\ndf.rename({'image_name':'image','comment': 'caption'},inplace=True,axis=1)\ndf['image'] = df['image'].map(lambda x:base_path / x.strip())\ndf['caption'] = df['caption'].map(lambda x:x.strip().lower())\ndf.head()\n\"\"\"","metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:30.512407Z","iopub.status.busy":"2023-09-27T06:36:30.512027Z","iopub.status.idle":"2023-09-27T06:36:32.218701Z","shell.execute_reply":"2023-09-27T06:36:32.217293Z","shell.execute_reply.started":"2023-09-27T06:36:30.512376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, val_df = train_test_split(df,test_size=0.1)\ntrain_df.reset_index(drop=True,inplace=True)\nval_df.reset_index(drop=True,inplace=True)\nprint(len(train_df),len(val_df))","metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:32.220831Z","iopub.status.busy":"2023-09-27T06:36:32.220387Z","iopub.status.idle":"2023-09-27T06:36:32.256739Z","shell.execute_reply":"2023-09-27T06:36:32.255686Z","shell.execute_reply.started":"2023-09-27T06:36:32.220797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = Dataset(train_df,train_tfms)\nval_ds = Dataset(val_df,valid_tfms)","metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:32.258834Z","iopub.status.busy":"2023-09-27T06:36:32.258478Z","iopub.status.idle":"2023-09-27T06:36:32.263973Z","shell.execute_reply":"2023-09-27T06:36:32.263062Z","shell.execute_reply.started":"2023-09-27T06:36:32.258803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Custom collate function\n---\n\n- allows for dynamic padding so the model doesn't have to process `max_len` sequences which would be just filled with pad tokens\n- instead, every batch is padded according to the longest sequence in the batch","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    image = [i[0] for i in batch]\n    input_ids = [i[1] for i in batch]\n    labels = [i[2] for i in batch]\n    image = torch.stack(image,dim=0)\n    input_ids = tokenizer.pad(\n        {'input_ids':input_ids},\n        padding='longest',\n        return_attention_mask=False,\n        return_tensors='pt'\n    )['input_ids']\n    labels = tokenizer.pad(\n        {'input_ids':labels},\n        padding='longest',\n        return_attention_mask=False,\n        return_tensors='pt'\n    )['input_ids']\n    mask = (input_ids!=tokenizer.pad_token_id).long()\n    labels[mask==0]=-100\n    return image, input_ids, labels","metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:32.266165Z","iopub.status.busy":"2023-09-27T06:36:32.265503Z","iopub.status.idle":"2023-09-27T06:36:32.274188Z","shell.execute_reply":"2023-09-27T06:36:32.273492Z","shell.execute_reply.started":"2023-09-27T06:36:32.266135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## How the data looks:\n---\n- every caption is a sequence of tokens, and as it is causal language modeling where the model predicts the next token, the labels are right shifted by 1 position.\n- every caption ends with the end of sentence token: eos_token (50256 : <|endoftext|>)\n- in GPT models, the pad tokens are same as the eos tokens, hence we also mask the pad tokens in the labels with -100 which are ignored by cross-entropy loss' default behaviour -- check `collate_fn` to see how I masked them. ","metadata":{}},{"cell_type":"code","source":"dl = torch.utils.data.DataLoader(train_ds,shuffle=True,batch_size=2,collate_fn=collate_fn)\n_,c,l = next(iter(dl))\nprint(c[0])\nprint(l[0])","metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:32.276230Z","iopub.status.busy":"2023-09-27T06:36:32.275598Z","iopub.status.idle":"2023-09-27T06:36:32.360675Z","shell.execute_reply":"2023-09-27T06:36:32.359221Z","shell.execute_reply.started":"2023-09-27T06:36:32.276198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model\n---\n\n- GPT2 on its own is a decoder-only model. So it understands only textual context. To create a model which also understands image context and applies it to the text, we use cross-attention.\n- Cross Attention: It is the second attention layer of the decoder block as per the [classic transformer encoder-decoder model](https://www.arxiv-vanity.com/papers/1706.03762/)\n- The query in cross-attention is the output of the previous causal attention layer in the decoder block. The key and value are the outputs of the corresponding encoder block i.e. the image context.\n- Since GPT2 doesn't have a cross-attention layer, I coded it as `GPT2CrossAttention` and made it a part of the usual decoder `GPT2Block`.\n- The encoder I used was ViT-base with patch_size=16 grabbed from `timm`.\n- ViT and GPT2 are extremely compatible with each other. They have the same amount of depth i.e. encoder, decoder blocks respectively. They have the same hidden size of 768 as well. So I didn't have to pool the encoder outputs to match the decoder hidden size. Less parameters yay!\n- GPT2 code I wrote was ofc based on what I learnt from [NanoGPT](https://github.com/karpathy/nanoGPT)\n\n### This is somewhat how the architecture looks now.\n![](https://i.imgur.com/fk68DMo.jpg)\n*I drew this :)*","metadata":{}},{"cell_type":"markdown","source":"## Causal Attention Block\n---","metadata":{}},{"cell_type":"code","source":"class GPT2Attention(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        self.embed_dim = config.embed_dim\n        self.n_heads = config.num_heads\n        assert self.embed_dim % self.n_heads == 0, 'embedding dimension by be divisible by number of heads'\n        self.head_size = self.embed_dim // self.n_heads\n        self.seq_len = config.seq_len\n        \n        self.c_attn = nn.Linear(self.embed_dim, self.head_size * self.n_heads * 3,bias=True)\n        self.scale = self.head_size ** -0.5\n        \n        self.register_buffer('mask',torch.tril(torch.ones(1,1,self.seq_len,self.seq_len)))\n        \n        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n        \n        self.attn_dropout = nn.Dropout(config.attention_dropout)\n        self.resid_dropout = nn.Dropout(config.residual_dropout)\n        \n        \n    def forward(self, x):\n        b,t,c = x.shape\n        # q,k,v shape individually: batch_size x seq_len x embed_dim\n        # we know that qk_t = q x k_t, where q=bxtxhead_dim, k_t=bxhead_timxt\n        q,k,v = self.c_attn(x).chunk(3,dim=-1)\n        q = q.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3) # batch x n_heads x seq_len x head_dim\n        k = k.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n        v = v.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n        \n        qk_t = (q@k.transpose(-2,-1)) * self.scale\n        qk_t = qk_t.masked_fill(self.mask[:,:,:t,:t]==0,float('-inf'))\n        qk_t = F.softmax(qk_t,dim=-1)\n        weights = self.attn_dropout(qk_t)\n        \n        attention = weights @ v # batch x n_heads x t x head_size\n        attention = attention.permute(0,2,1,3).contiguous().view(b,t,c) # batch x t x embed_dim\n        \n        out = self.c_proj(attention)\n        out = self.resid_dropout(out)\n        \n        return out","metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:32.362526Z","iopub.status.busy":"2023-09-27T06:36:32.362154Z","iopub.status.idle":"2023-09-27T06:36:32.375632Z","shell.execute_reply":"2023-09-27T06:36:32.374555Z","shell.execute_reply.started":"2023-09-27T06:36:32.362484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cross Attention Block\n---\n- Notice how I initialized the parameters with mean=0. and std=0.02. This is an OpenAI trick they used while training GPT2 as well.","metadata":{}},{"cell_type":"code","source":"class GPT2CrossAttention(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        self.embed_dim = config.embed_dim\n        self.n_heads = config.num_heads\n        assert self.embed_dim % self.n_heads == 0, 'embedding dimension by be divisible by number of heads'\n        self.head_size = self.embed_dim // self.n_heads\n        self.seq_len = config.seq_len\n        \n        self.q = nn.Linear(self.embed_dim,self.embed_dim)\n        self.k = nn.Linear(self.embed_dim,self.embed_dim)\n        self.v = nn.Linear(self.embed_dim,self.embed_dim)\n        self.scale = self.head_size ** -0.5\n        \n        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n        \n        self.attn_dropout = nn.Dropout(config.attention_dropout)\n        self.resid_dropout = nn.Dropout(config.residual_dropout)\n        \n        self.apply(self._init_weights)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        \n        \n    def forward(self, q,k,v):\n        b,t,c = q.shape\n        \n        q = self.q(q)\n        k = self.k(k)\n        v = self.v(v)\n        \n        q = q.view(b,q.size(1),self.n_heads,self.head_size).permute(0,2,1,3) # batch x n_heads x seq_len x head_dim\n        k = k.view(b,k.size(1),self.n_heads,self.head_size).permute(0,2,1,3)\n        v = v.view(b,v.size(1),self.n_heads,self.head_size).permute(0,2,1,3)\n        \n        qk_t = (q@k.transpose(-2,-1)) * self.scale\n        qk_t = F.softmax(qk_t,dim=-1)\n        weights = self.attn_dropout(qk_t)\n        \n        attention = weights @ v # batch x n_heads x t x head_size\n        attention = attention.permute(0,2,1,3).contiguous().view(b,t,c) # batch x t x embed_dim\n        \n        out = self.c_proj(attention)\n        out = self.resid_dropout(out)\n        \n        return out","metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:32.377746Z","iopub.status.busy":"2023-09-27T06:36:32.377399Z","iopub.status.idle":"2023-09-27T06:36:32.393228Z","shell.execute_reply":"2023-09-27T06:36:32.392279Z","shell.execute_reply.started":"2023-09-27T06:36:32.377714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feed Forward Block\n---","metadata":{}},{"cell_type":"code","source":"class GPT2MLP(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        self.embed_dim = config.embed_dim\n        self.mlp_ratio = config.mlp_ratio\n        self.mlp_dropout = config.mlp_dropout\n        \n        self.c_fc = nn.Linear(self.embed_dim,self.embed_dim*self.mlp_ratio)\n        self.c_proj = nn.Linear(self.embed_dim*self.mlp_ratio,self.embed_dim)\n        self.act = nn.GELU()\n        self.dropout = nn.Dropout(self.mlp_dropout)\n        \n    def forward(self,x):\n        x = self.c_fc(x)\n        x = self.act(x)\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x","metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:32.395071Z","iopub.status.busy":"2023-09-27T06:36:32.394734Z","iopub.status.idle":"2023-09-27T06:36:32.404064Z","shell.execute_reply":"2023-09-27T06:36:32.403373Z","shell.execute_reply.started":"2023-09-27T06:36:32.395040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decoder Block\n---\n- with added cross-attention and pre-normalization","metadata":{}},{"cell_type":"code","source":"class GPT2Block(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        self.embed_dim = config.embed_dim\n        self.ln_1 = nn.LayerNorm(self.embed_dim)\n        self.attn = GPT2Attention(config)\n        self.ln_2 = nn.LayerNorm(self.embed_dim)\n        self.mlp = GPT2MLP(config)\n        self.ln_3 = nn.LayerNorm(self.embed_dim)\n        self.cross_attn = GPT2CrossAttention(config)\n        \n    def forward(self,x,enc_out):\n        x = x+self.attn(self.ln_1(x))\n        x = x+self.cross_attn(self.ln_2(x),enc_out,enc_out)\n        x = x+self.mlp(self.ln_3(x))\n        return x","metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:32.410217Z","iopub.status.busy":"2023-09-27T06:36:32.409138Z","iopub.status.idle":"2023-09-27T06:36:32.417659Z","shell.execute_reply":"2023-09-27T06:36:32.416743Z","shell.execute_reply.started":"2023-09-27T06:36:32.410186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The main model\n---\n- creating ViT model via timm\n- added embedding code and right forward pass for the ViT model as per [their code.](https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py)\n- each encoder output is passed to the decoder input and to the next encoder input as well.\n- the final decoder outputs are passed through a head block to generate logits as per the vocab size.\n- loss is calculated using cross-entropy if labels are present else, logits for the final token in the sequence are returned for generation.\n- `pretrained_layers_trainable`: freezes/unfreezes ViT and GPT2 pretrained layers\n- `unfreeze_gpt_layers`: unfreezes GPT2 layers only\n- `from_pretrained`: loads GPT2 weights via huggingface gpt2\n- `generate`: generates caption, sampling via `torch.multinomial` or deterministic via `argmax` with `temperature` control.","metadata":{}},{"cell_type":"code","source":"class VisionGPT2Model(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        \n        self.config = config\n        \n        vit = create_model('vit_base_patch16_224',pretrained=True,num_classes=0)\n        self.patch_embed = vit.patch_embed\n        num_patches = self.patch_embed.num_patches\n        \n        self.cls_token = vit.cls_token\n        embed_len = num_patches + vit.num_prefix_tokens\n        self.pos_embed = vit.pos_embed\n        self.pos_drop = nn.Dropout(p=0.)\n        \n        self.blocks = nn.ModuleList([vit.blocks[i] for i in range(config.depth)])\n        \n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size,config.embed_dim),\n            wpe = nn.Embedding(config.seq_len,config.embed_dim),\n            drop = nn.Dropout(config.emb_dropout),\n            h = nn.ModuleList([GPT2Block(config) for _ in range(config.depth)]),\n            ln_f = nn.LayerNorm(config.embed_dim)\n        ))\n        self.lm_head = nn.Linear(config.embed_dim,config.vocab_size,bias=False)\n        self.transformer.wte.weight = self.lm_head.weight\n        \n    def _pos_embed(self,x):\n        pos_embed = self.pos_embed\n        x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n        x = x + pos_embed\n        return self.pos_drop(x)\n    \n    def pretrained_layers_trainable(self,trainable=False):\n        layers = [\n            self.cls_token, self.patch_embed, self.pos_embed, self.blocks,\n            self.transformer.wte, self.transformer.wpe,\n            self.transformer.ln_f, self.lm_head\n        ]\n        gpt_layers = [[\n            self.transformer.h[i].ln_1,self.transformer.h[i].ln_2,\n            self.transformer.h[i].attn,self.transformer.h[i].mlp\n        ] for i in range(self.config.depth)]\n        for l in gpt_layers:\n            layers.extend(l)\n        \n        for layer in layers:\n            if not isinstance(layer,nn.Parameter):\n                for p in layer.parameters():\n                    p.requires_grad = trainable\n            else:\n                layer.requires_grad = trainable\n                \n        total_frozen_params = sum([p.numel() for p in self.parameters() if not p.requires_grad])\n        print(f'{total_frozen_params=}')\n        \n    def unfreeze_gpt_layers(self,):\n        gpt_layers = [[\n            self.transformer.h[i].ln_1,self.transformer.h[i].ln_2,\n            self.transformer.h[i].attn,self.transformer.h[i].mlp\n        ] for i in range(self.config.depth)]\n        flatten = []\n        for l in gpt_layers:\n            flatten.extend(l)\n            \n        for layer in flatten:\n            if not isinstance(layer,nn.Parameter):\n                for p in layer.parameters():\n                    p.requires_grad = True\n            else:\n                layer.requires_grad = True\n        \n    @classmethod    \n    def from_pretrained(self,config):\n        model = VisionGPT2Model(config)\n        sd = model.state_dict()\n        keys = sd.keys()\n        ignore_matches = ['blocks.','cross_attn.','ln_3','cls_token','pos_embed','patch_embed.','.attn.mask']\n        vit_keys = [key for key in keys if any(match in key for match in ignore_matches)]\n        gpt_keys = [key for key in keys if key not in vit_keys]\n        \n        gpt2_small = GPT2LMHeadModel.from_pretrained('gpt2')\n        sd_hf = gpt2_small.state_dict()\n        hf_keys = sd_hf.keys()\n        hf_keys = [k for k in hf_keys if not k.endswith('.attn.masked_bias')]\n        hf_keys = [k for k in hf_keys if not k.endswith('.attn.bias')]\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n        \n        for k in hf_keys:\n            if any(match in k for match in ignore_matches):\n                continue\n            if any(k.endswith(w) for w in transposed):\n                assert sd_hf[k].shape[::-1] == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k].t())\n            else:\n                assert sd_hf[k].shape == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k])\n            \n        model.load_state_dict(sd)\n        \n        return model\n    \n    def forward(self,image,input_ids,labels=None):\n        \n        image = self.patch_embed(image)\n        image = self._pos_embed(image)\n        \n        token_embeddings = self.transformer.wte(input_ids) # batch x seq_len\n        pos_embs = torch.arange(0,input_ids.size(1)).to(input_ids.device)\n        positional_embeddings = self.transformer.wpe(pos_embs)\n        input_ids = self.transformer.drop(token_embeddings+positional_embeddings)\n        \n        for i in range(self.config.depth):\n            image = self.blocks[i](image)\n            input_ids = self.transformer.h[i](input_ids, image)\n        \n        input_ids = self.transformer.ln_f(input_ids)\n        \n        if labels is not None:\n            lm_logits = self.lm_head(input_ids)\n            loss = F.cross_entropy(lm_logits.view(-1, lm_logits.shape[-1]), labels.view(-1))\n            return loss\n        \n        lm_logits = self.lm_head(input_ids[:,[-1],:])\n        return lm_logits\n    \n    def generate(self,image,sequence,max_tokens=50,temperature=1.0,deterministic=False):\n        for _ in range(max_tokens):\n            out = self(image,sequence)\n            out = out[:,-1,:] / temperature\n            probs = F.softmax(out,dim=-1)\n            if deterministic:\n                next_token = torch.argmax(probs,dim=-1,keepdim=True)\n            else:\n                next_token = torch.multinomial(probs,num_samples=1)\n            sequence = torch.cat([sequence,next_token],dim=1)\n            if next_token.item() == tokenizer.eos_token_id:\n                break\n            \n        return sequence.cpu().flatten()","metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:32.430824Z","iopub.status.busy":"2023-09-27T06:36:32.430084Z","iopub.status.idle":"2023-09-27T06:36:32.461289Z","shell.execute_reply":"2023-09-27T06:36:32.460315Z","shell.execute_reply.started":"2023-09-27T06:36:32.430769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training\n---\n- the pretrained layers are initially frozen as I need to train the cross attention layers first\n- in the following epochs, GPT2 is unfreezed and trained, in the final few epochs, the ViT is unfreezed as well.\n- optimizer: Adam\n- scheduler: OneCycleLR\n- mixed-precision fp16 training with autocast and grad-scaler in torch\n- metrics: cross-entropy loss and perplexity = e^loss, both lower the better\n- best model is saved based on validation perplexity and the same is loaded while generating captions.\n\n### Generation\n\n- GPT2 generally requires context before generating anything, since for image captioning we can't really provide an initial context except the image itself, the initial context we provide to GPT is just `[50256]` i.e `<|endoftext|>` which is also the beginning of sentence token in GPT. For other models such as OPT it is `</s>`. This one token acts as the initial context.","metadata":{}},{"cell_type":"code","source":"class Trainer:\n    def __init__(self,model_config,train_config, dls):\n        \n        self.train_config = train_config\n        self.model_config = model_config\n        self.device = self.train_config.device\n        \n        self.model = VisionGPT2Model.from_pretrained(model_config).to(self.device)\n        self.model.pretrained_layers_trainable(trainable=False)\n        \n        print(f'trainable parameters: {sum([p.numel() for p in self.model.parameters() if p.requires_grad])}')\n        \n        self.tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        \n        self.scaler = GradScaler()\n        \n        self.train_dl, self.val_dl = dls\n        \n        total_steps = len(self.train_dl)\n        \n        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.train_config.lr / 25.)\n        self.sched = torch.optim.lr_scheduler.OneCycleLR(\n            self.optim,\n            max_lr=self.train_config.lr,\n            epochs=self.train_config.epochs,\n            steps_per_epoch=total_steps\n        )\n        \n#         self.sched = get_linear_schedule_with_warmup(self.optim,num_warmup_steps=0,num_training_steps=total_steps)\n        \n        self.metrics = pd.DataFrame()\n        self.metrics[['train_loss','train_perplexity','val_loss','val_perplexity']] = None\n        \n        self.gen_tfms = A.Compose([\n            A.Resize(224,224),\n            A.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5],always_apply=True),\n            ToTensorV2()\n        ])\n            \n        \n    def save_model(self,):\n        self.train_config.model_path.mkdir(exist_ok=True)\n        sd = self.model.state_dict()\n        torch.save(sd,self.train_config.model_path/'captioner.pt')\n        \n        \n    def load_best_model(self,):\n        sd = torch.load(self.train_config.model_path/'captioner.pt')\n        self.model.load_state_dict(sd)\n    \n    \n    def train_one_epoch(self,epoch):\n        \n        prog = tqdm(self.train_dl,total=len(self.train_dl))\n        \n        running_loss = 0.\n        \n        for image, input_ids, labels in prog:\n            \n            with autocast():\n                image = image.to(self.device)\n                input_ids = input_ids.to(self.device)\n                labels = labels.to(self.device)\n                \n                loss = self.model(image,input_ids,labels)\n                \n                self.scaler.scale(loss).backward()\n                self.scaler.step(self.optim)\n                self.scaler.update()\n                self.sched.step()\n                self.optim.zero_grad(set_to_none=True)\n                \n                running_loss += loss.item()\n                \n                prog.set_description(f'train loss: {loss.item():.3f}')\n                \n            del image, input_ids, labels, loss\n            \n        train_loss = running_loss / len(self.train_dl)\n        train_pxp = np.exp(train_loss)\n        \n        self.metrics.loc[epoch,['train_loss','train_perplexity']] = (train_loss,train_pxp)\n        \n        \n    @torch.no_grad()\n    def valid_one_epoch(self,epoch):\n        \n        prog = tqdm(self.val_dl,total=len(self.val_dl))\n        \n        running_loss = 0.\n        \n        for image, input_ids, labels in prog:\n            \n            with autocast():\n                image = image.to(self.device)\n                input_ids = input_ids.to(self.device)\n                labels = labels.to(self.device)\n                \n                loss = self.model(image,input_ids,labels)\n                running_loss += loss.item()\n                \n                prog.set_description(f'valid loss: {loss.item():.3f}')\n                \n            del image, input_ids, labels, loss\n            \n        val_loss = running_loss / len(self.val_dl)\n        val_pxp = np.exp(val_loss)\n        \n        self.metrics.loc[epoch,['val_loss','val_perplexity']] = (val_loss,val_pxp)\n        \n        return val_pxp\n        \n        \n    def clean(self):\n        gc.collect()\n        torch.cuda.empty_cache()\n       \n    \n    def fit(self,):\n        \n        best_pxp = 1e9\n        best_epoch = -1\n        prog = tqdm(range(self.train_config.epochs))\n        \n        for epoch in prog:\n            \n            if epoch == self.train_config.freeze_epochs_gpt:\n                self.model.unfreeze_gpt_layers()\n                print('unfreezing GPT2 entirely...')\n                \n            if epoch == self.train_config.freeze_epochs_all:\n                self.model.pretrained_layers_trainable(trainable=True)\n            \n            self.model.train()\n            prog.set_description('training')\n            self.train_one_epoch(epoch)\n            self.clean()\n            \n            self.model.eval()\n            prog.set_description('validating')\n            pxp = self.valid_one_epoch(epoch)\n            self.clean()\n            \n            print(self.metrics.tail(1))\n            \n            if pxp < best_pxp:\n                best_pxp = pxp\n                best_epoch = epoch\n                print('saving best model...')\n                self.save_model()\n                \n        return {\n            'best_perplexity': best_pxp,\n            'best_epoch': best_epoch\n        }\n           \n        \n    @torch.no_grad()\n    def generate_caption(self,image,max_tokens=50,temperature=1.0,deterministic=False):\n        \n        self.model.eval()\n        \n        image = Image.open(image).convert('RGB')\n        image = np.array(image)\n        image = self.gen_tfms(image=image)['image']\n        image = image.unsqueeze(0).to(self.device)\n        sequence = torch.ones(1,1).to(device=self.device).long() * self.tokenizer.bos_token_id\n        \n        caption = self.model.generate(\n            image,\n            sequence,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            deterministic=deterministic\n        )\n        caption = self.tokenizer.decode(caption.numpy(),skip_special_tokens=True)\n        \n        return caption","metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:32.464621Z","iopub.status.busy":"2023-09-27T06:36:32.464257Z","iopub.status.idle":"2023-09-27T06:36:32.493149Z","shell.execute_reply":"2023-09-27T06:36:32.492142Z","shell.execute_reply.started":"2023-09-27T06:36:32.464597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_config = SimpleNamespace(\n    vocab_size = 50_257,\n    embed_dim = 768,\n    num_heads = 12,\n    seq_len = 1024,\n    depth = 12,\n    attention_dropout = 0.1,\n    residual_dropout = 0.1,\n    mlp_ratio = 4,\n    mlp_dropout = 0.1,\n    emb_dropout = 0.1,\n)\ntrain_config = SimpleNamespace(\n    epochs = 5,\n    freeze_epochs_gpt = 1,\n    freeze_epochs_all = 2,\n    lr = 1e-4,\n    device = 'cuda',\n    model_path = Path('captioner'),\n    batch_size = 32\n)","metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:32.495776Z","iopub.status.busy":"2023-09-27T06:36:32.495097Z","iopub.status.idle":"2023-09-27T06:36:32.505104Z","shell.execute_reply":"2023-09-27T06:36:32.504120Z","shell.execute_reply.started":"2023-09-27T06:36:32.495744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dl = torch.utils.data.DataLoader(train_ds,batch_size=train_config.batch_size,shuffle=True,pin_memory=True,num_workers=2,persistent_workers=True,collate_fn=collate_fn)\nval_dl = torch.utils.data.DataLoader(val_ds,batch_size=train_config.batch_size,shuffle=False,pin_memory=True,num_workers=2,persistent_workers=True,collate_fn=collate_fn)","metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:32.507109Z","iopub.status.busy":"2023-09-27T06:36:32.506737Z","iopub.status.idle":"2023-09-27T06:36:32.515883Z","shell.execute_reply":"2023-09-27T06:36:32.514940Z","shell.execute_reply.started":"2023-09-27T06:36:32.507079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(model_config,train_config,(train_dl,val_dl))","metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:36:32.517740Z","iopub.status.busy":"2023-09-27T06:36:32.517357Z","iopub.status.idle":"2023-09-27T06:37:08.792377Z","shell.execute_reply":"2023-09-27T06:37:08.791224Z","shell.execute_reply.started":"2023-09-27T06:36:32.517711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.fit()","metadata":{"execution":{"iopub.execute_input":"2023-09-27T06:37:08.796343Z","iopub.status.busy":"2023-09-27T06:37:08.796031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results\n---","metadata":{}},{"cell_type":"code","source":"trainer.metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(trainer.metrics['train_loss'],color='red',label='train loss')\nplt.plot(trainer.metrics['val_loss'],color='orange',label='valid loss')\nplt.title('loss, lower=better')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(trainer.metrics['train_perplexity'],color='blue',label='train perplexity')\nplt.plot(trainer.metrics['val_perplexity'],color='lightblue',label='valid perplexity')\nplt.title('perplexity, lower=better')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predictions\n---","metadata":{}},{"cell_type":"code","source":"trainer.load_best_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(50):\n    det = False\n    test = val_df.sample(n=1).values[0]\n    test_img, test_caption = test[0],test[1]\n    plt.imshow(Image.open(test_img).convert('RGB'))\n    t = np.random.uniform(0.5,1.5)\n    if i > 40:\n        det = True\n    gen_caption = trainer.generate_caption(test_img,temperature=t,deterministic=det)\n    plt.title(f\"actual: {test_caption}\\nmodel: {gen_caption}\\ntemp: {t} deterministic generation: {det}\")\n    plt.axis('off')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n---\n\n- Dataset options which are a good start: Flickr8k, Flickr30k, MS-COCO\n- The generations are good not great, can definitely train for a lot longer for better results\n- Additional decoding strategies like beam, contrastive could've been implemented\n- All of this can be done using HuggingFace `VisionEncoderDecoderModel` and their `Seq2SeqTrainer`, here's a [tutorial](https://ankur3107.github.io/blogs/the-illustrated-image-captioning-using-transformers/)\n- I wanted to implement this from scratch since I like exploring model architectures! Check out my [Github](https://github.com/shreydan) for more stuff like this.\n- Follow me on [Kaggle](https://kaggle.com/shreydan) if you haven't already!\n- Upvote (it really helps), comment your thoughts and lemme know what else I could've done to improve or bring any errors I made to my attention (pun intended).\n- See you around!\n\n> Attention is all you need.\n---\nPsalm 32:8","metadata":{}}]}